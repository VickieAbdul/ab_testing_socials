# -*- coding: utf-8 -*-
"""ab_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j0N1IHq1_o9D7A3rD0-hhPWPDLN--Ova

## 1. Define the project objective

Before we start coding, we need to decide what we are testing.

We'll compare different ad campaigns within the xyz_campaign_id to see which one performed better.

**How do we measure a successful campaign?**

We'll compare the campaigns based on:

- Click-Through Rate (CTR): More clicks = better engagement.

- Conversion Rate (CR): More purchases = more success.

- Cost Per Conversion (CPCo): Lower cost = better efficiency.

**Hypothesis**

- Null Hypothesis (H₀): There is no difference in performance between xyz_campaign adds.

- lternative Hypothesis (Ha): One campaign performs significantly better than the other.

## 2. Load and Explore the Data
"""

# import library for reading data
import pandas as pd

# read data into notebook
data = pd.read_csv('conversion_data.csv')

# inspect the first few rows
data.head()

# check for missing values
data.isnull().sum()

# check data types for each column
data.info()

# Check for duplicates
print(data.duplicated().sum())

# get basic statistics
data.describe()

"""## 3. Compare Campaign Performance"""

# summary of xyz campaign stats
xyz_campaign_stats = data.groupby('xyz_campaign_id').agg(
    Impressions=('Impressions', 'sum'),
    Clicks=('Clicks', 'sum'),
    Spent=('Spent', 'sum'),
    Total_Conversions=('Total_Conversion', 'sum'),
    Approved_Conversions=('Approved_Conversion', 'sum')
).reset_index()

# display the summary
xyz_campaign_stats

# let's add click though rates and conversion rates to the table for better interpretation
xyz_campaign_stats['CTR (%)'] = (xyz_campaign_stats['Clicks'] / xyz_campaign_stats['Impressions']) * 100
xyz_campaign_stats['CR (%)'] = (xyz_campaign_stats['Approved_Conversions'] / xyz_campaign_stats['Clicks']) * 100
xyz_campaign_stats["CPC"] = xyz_campaign_stats["Spent"] / xyz_campaign_stats["Total_Conversions"]

# Display the campaign summary
xyz_campaign_stats

# select the needed columns for interpretation
xyz_campaign_stats[['xyz_campaign_id', 'CTR (%)', 'CR (%)', 'CPC']]

"""## Key Insights from the Data

1. Best Engagement (CTR)  is Campaign 936.

   It had 0.0244% CTR (Highest click-through rate).

   This means more people clicked on this campaign compared to others.

2. Best Conversion (CR) is Campaign 916

   It had 21.24% CR (Best at converting clicks into actual customers)

   This means it efficiently converted users into customers.

3. Most Cost-Effective (CPC) is Campaign 916

   It had $2.58 per conversion (Lowest cost per conversion)

   Meaning, it spent the least amount to get a conversion.


From this, we can deduce that Campaign 916 is the best overall campaign because:
- Highest Conversion Rate (21.24%). More people took action.
- Lowest Cost per Conversion ($2.58). Most cost-efficient.
- Decent CTR (0.0234%). Not the highest, but effective.

We know that we cannot stop here without proving that there is a statistical significance to back up our claims above.

Let's proceed with the actual test.

## 4. Prepare Data for A/B Testing

We will focus on two key groups for the test:

- Campaign 916 vs. Campaign 936

- Campaign 916 vs. Campaign 1178
"""

# calculate non-conversions
xyz_campaign_stats['Non_Conversions'] = xyz_campaign_stats['Clicks'] - xyz_campaign_stats['Total_Conversions']

# display contingency table
contingency_table = xyz_campaign_stats[['Total_Conversions', 'Non_Conversions']]

contingency_table

# import library for performing chi-square test
from scipy.stats import chi2_contingency

chi2, p, dof, expected = chi2_contingency(contingency_table)

print(f"\nChi-Square Statistic: {chi2}")
print(f"P-value: {p}")
print(f"Degrees of Freedom: {dof}")

"""## Interpretation of Results

1. Chi-Square Statistic (1195.1607839746687)

This high value indicates a huge difference between the observed and expected conversions.

2. P-Value (2.979430548404177e-260)

Our p-value (2.98 * 10⁻²⁶⁰) is extremely small, and very well below the significance level.

This means that the difference between campaigns is statistically significant. So, we can confidently reject the null hypothesis that all campaigns perform the same.
"""

# convert expected values to a DataFrame for better readability
expected_data = pd.DataFrame(expected, columns=["Expected_Conversions", "Expected_Non_Conversions"])
expected_data.index = ["Campaign 916", "Campaign 936", "Campaign 1178"]

# display expected values
expected_data

"""We can plot a graph of our expected conversions from this campaign vs the actual conversions."""

# import libraries
import matplotlib.pyplot as plt
import numpy as np

# create campaign names
campaigns = ["Campaign 916", "Campaign 936", "Campaign 1178"]

# observed conversions from the original data
observed_conversions = contingency_table["Total_Conversions"].values

# expected conversions from chi-square test
expected_conversions = expected_data["Expected_Conversions"].values

# create bar width and positions
bar_width = 0.4
x = np.arange(len(campaigns))

'''
To compare observed and expected conversions across campaigns, I used a grouped bar chart.
The bars are positioned using np.arange() to avoid overlap, ensuring clear visualization
'''

# creating the bar chart
plt.figure(figsize=(10, 5))
plt.bar(x - bar_width/2, observed_conversions, bar_width, label="Observed Conversions", color="blue")
plt.bar(x + bar_width/2, expected_conversions, bar_width, label="Expected Conversions", color="orange")

# labels and title
plt.xlabel("Campaigns")
plt.ylabel("Number of Conversions")
plt.title("Observed vs. Expected Conversions")
plt.xticks(x, campaigns)
plt.legend();

"""## Conclusion from the A/B Test Analysis

The Observed Conversions vs. Expected Conversions comparison shows great differences between the campaigns.

1. Campaign 916: The observed (58) is substantially higher than expected (9.66), and this shows the campaign performed way better than one would expect.

2. Campaign 936: Observed (537) is significantly higher compared to expected (169.67), and the engagement and responsiveness are good here.

3. Campaign 1178: Noticed conversions (2669) fall below expectations (3084.66), so it appears that this campaign had less success statistically than would have been anticipated.

**Statistical Validation**

The Chi-Square test (p-value = 2.98 * 10⁻²⁶⁰) confirms differences between observed vs. expected conversions to be statistically significant, i.e., the campaigns were not equal by chance.

**Key Takeaways**

- Campaigns 916 and 936 exceeded expectations, making them top contenders for further scaling.

- Campaign 1178 underperformed against expectations, suggesting the need for optimization (e.g., better audience targeting, ad creative optimization).

- This analysis provides evidence-based suggestions for future ad campaign optimization and overall conversion rate optimization.



"""